{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import copy\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, random_split, \\\nTensorDataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-28T05:48:49.437014Z","iopub.execute_input":"2023-04-28T05:48:49.437428Z","iopub.status.idle":"2023-04-28T05:48:49.443299Z","shell.execute_reply.started":"2023-04-28T05:48:49.437391Z","shell.execute_reply":"2023-04-28T05:48:49.442178Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,n_features,hidden_dim):\n        \n        super().__init__()\n        self.hidden_dim=hidden_dim\n        self.n_features=n_features\n        self.hidden=None\n        self.basic_rnn=nn.GRU(self.n_features,\n                             self.hidden_dim,\n                             batch_first=True)\n        \n    def forward(self,x):\n        rnn_out,self.hidden=self.basic_rnn(x)\n        return rnn_out\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.473648Z","iopub.execute_input":"2023-04-28T05:48:49.474838Z","iopub.status.idle":"2023-04-28T05:48:49.482708Z","shell.execute_reply.started":"2023-04-28T05:48:49.474750Z","shell.execute_reply":"2023-04-28T05:48:49.481397Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"full_seq = (torch.tensor([[-1, -2], [-1, 3], [1, 4], [1, -5]])\n.float()\n.view(1, 4, 2))\nsource_seq = full_seq[:, :2] # first two corners\ntarget_seq = full_seq[:, 2:] # last two corners","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.508698Z","iopub.execute_input":"2023-04-28T05:48:49.509130Z","iopub.status.idle":"2023-04-28T05:48:49.515208Z","shell.execute_reply.started":"2023-04-28T05:48:49.509089Z","shell.execute_reply":"2023-04-28T05:48:49.514221Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(21)\nencoder = Encoder(n_features=2, hidden_dim=2)\nhidden_seq = encoder(source_seq) # output is N, L, F\nhidden_final = hidden_seq[:, -1:]\n# takes last hidden state\nhidden_final,hidden_seq","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.550858Z","iopub.execute_input":"2023-04-28T05:48:49.551554Z","iopub.status.idle":"2023-04-28T05:48:49.562838Z","shell.execute_reply.started":"2023-04-28T05:48:49.551515Z","shell.execute_reply":"2023-04-28T05:48:49.561368Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 0.5242, -0.4707]]], grad_fn=<SliceBackward0>),\n tensor([[[ 0.0122,  0.3053],\n          [ 0.5242, -0.4707]]], grad_fn=<TransposeBackward1>))"},"metadata":{}}]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \n    def __init__(self,n_features,hidden_dim):\n        \n        super().__init__()\n        \n        self.hidden_dim=hidden_dim\n        \n        self.n_features=n_features\n        \n        self.hidden=None\n        \n        self.basic_rnn=nn.GRU(self.n_features,\n                             self.hidden_dim,\n                             batch_first=True)\n        self.regression=nn.Linear(self.hidden_dim,\n                                 self.n_features)\n    def init_hidden(self,hidden_seq):\n        \n        hidden_final=hidden_seq[:,-1:]\n        \n        self.hidden=hidden_final.permute(1,0,2)\n        \n    def forward(self,x):\n        \n        batch_first_output , self.hidden=self.basic_rnn(x,self.hidden)\n        \n        last_output=batch_first_output[:,-1:]\n        \n        out=self.regression(last_output)\n        \n        return out.view(-1,1,self.n_features)\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.586742Z","iopub.execute_input":"2023-04-28T05:48:49.587432Z","iopub.status.idle":"2023-04-28T05:48:49.596250Z","shell.execute_reply.started":"2023-04-28T05:48:49.587392Z","shell.execute_reply":"2023-04-28T05:48:49.595016Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(21)\ndecoder=Decoder(2,2)\n\ndecoder.init_hidden(hidden_seq)\ninputs=source_seq[:,-1:]\n\nprint('Inputs :',inputs)\ntarget_len=2\nfor i in range(target_len):\n    print(f'Hidden : {decoder.hidden}')\n    out=decoder(inputs)\n    print(f'outputs :{out}')\n    inputs=out","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.623338Z","iopub.execute_input":"2023-04-28T05:48:49.624196Z","iopub.status.idle":"2023-04-28T05:48:49.636339Z","shell.execute_reply.started":"2023-04-28T05:48:49.624144Z","shell.execute_reply":"2023-04-28T05:48:49.635158Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Inputs : tensor([[[-1.,  3.]]])\nHidden : tensor([[[ 0.5242, -0.4707]]], grad_fn=<PermuteBackward0>)\noutputs :tensor([[[-0.3348,  0.4237]]], grad_fn=<ViewBackward0>)\nHidden : tensor([[[ 0.6296, -0.8031]]], grad_fn=<StackBackward0>)\noutputs :tensor([[[-0.1195,  0.4348]]], grad_fn=<ViewBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initial hidden state will be encoder's final hidden state\ndecoder.init_hidden(hidden_seq)\n# Initial data point is the last element of source sequence\ninputs = source_seq[:, -1:]\n\ntarget_len=2\nfor i in range(target_len):\n    print(f'Hidden : {decoder.hidden}')\n    out=decoder(inputs)\n    print(f'outputs :{out}')\n    inputs=target_seq[:,i:i+1]\n    print(f'i :{target_seq[:,i:i+1]}')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.660251Z","iopub.execute_input":"2023-04-28T05:48:49.660927Z","iopub.status.idle":"2023-04-28T05:48:49.670573Z","shell.execute_reply.started":"2023-04-28T05:48:49.660880Z","shell.execute_reply":"2023-04-28T05:48:49.669383Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Hidden : tensor([[[ 0.5242, -0.4707]]], grad_fn=<PermuteBackward0>)\noutputs :tensor([[[-0.3348,  0.4237]]], grad_fn=<ViewBackward0>)\ni :tensor([[[1., 4.]]])\nHidden : tensor([[[ 0.6296, -0.8031]]], grad_fn=<StackBackward0>)\noutputs :tensor([[[-0.2940,  0.3910]]], grad_fn=<ViewBackward0>)\ni :tensor([[[ 1., -5.]]])\n","output_type":"stream"}]},{"cell_type":"code","source":"decoder.init_hidden(hidden_seq)\n\ninputs=source_seq[:,-1:]\n\nteacher_forcing_prob=0.5\ntarget_len=2\nfor i in range(target_len):\n    print(f'Hidden state :{decoder.hidden}')\n    out=decoder(inputs)\n    print(f'Output : {out}')\n    \n    if torch.randn(1)<=teacher_forcing_prob:\n        inputs=target_seq[:,i:i+1]\n        \n    else:\n        inputs=out","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.704678Z","iopub.execute_input":"2023-04-28T05:48:49.705081Z","iopub.status.idle":"2023-04-28T05:48:49.716188Z","shell.execute_reply.started":"2023-04-28T05:48:49.705046Z","shell.execute_reply":"2023-04-28T05:48:49.715184Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Hidden state :tensor([[[ 0.5242, -0.4707]]], grad_fn=<PermuteBackward0>)\nOutput : tensor([[[-0.3348,  0.4237]]], grad_fn=<ViewBackward0>)\nHidden state :tensor([[[ 0.6296, -0.8031]]], grad_fn=<StackBackward0>)\nOutput : tensor([[[-0.1195,  0.4348]]], grad_fn=<ViewBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    \n    def __init__(self,\n                encoder,\n                decoder,\n                input_len,\n                target_len,\n                teacher_forcing_prob=0.5):\n        super().__init__()\n        self.encoder=encoder\n        self.decoder=decoder\n        self.input_len=input_len\n        self.target_len=target_len\n        self.teacher_forcing_prob=teacher_forcing_prob\n        self.outputs=None\n        \n    def init_outputs(self,batch_size):\n        #print('\\n\\nInitialization of outputs ')\n        device = next(self.parameters()).device\n        # N---->batch_size\n        #L----->target_len\n        #F----->ecnoder_features\n        print(f'\\n\\nBatch_size ->(N) : {batch_size}\\nInput_length ->(L) : {self.target_len} \\nEncoder_feature-->(F) : {self.encoder.n_features}')\n        self.outputs=torch.zeros(batch_size,self.target_len,\n                                 self.encoder.n_features).to(device)\n        \n    def store_outputs(self,i,out):\n        print('\\n\\nStore_outputs')\n        self.outputs[:,i:i+1,:]=out\n        print(f'Full Output : ',out)\n        print('Iteration', i)\n        print(f'Storing output from {i} : {i+1}')\n        print(f'Stored outputs :{self.outputs[:,i:i+1]}')\n        \n    def forward(self,x):\n        print('\\n\\nforward')\n        #splits the data in source and target sequence\n        #the target seq will be empty in testing mode\n        # N,L,F\n        #batch_size=x.shape[0]\n        #x=x.view(batch_size,-1,self.input_len+self.target_len)\n        \n        source_seq=x[:,:self.input_len,:]\n        print(f'Source Seq : {source_seq}')\n        target_seq=x[:,self.input_len:,:]\n        print(f'target Seq : {target_seq}')\n        self.init_outputs(x.shape[0])\n        \n        hidden_seq=self.encoder(source_seq)\n        print(f'Hidden_seq Of encoder : {hidden_seq}')\n        self.decoder.init_hidden(hidden_seq)\n        \n        dec_inputs=source_seq[:,-1:,:]\n        print('\\n\\n***************Decoder Iteration******************\\n')\n        for i in range(self.target_len):\n            \n            out=self.decoder(dec_inputs)\n            \n            self.store_outputs(i,out)\n            \n            prob=self.teacher_forcing_prob\n            \n            if not self.training:\n                prob=0\n                \n            if torch.rand(1) <= prob:\n                \n                dec_inputs=target_seq[:,i:i+1,:]\n                #print(f'Decoder Ouputs : {target_seq[:,i:i+1,:]}')\n            else:\n                dec_inputs=out\n        \n        return self.outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.744592Z","iopub.execute_input":"2023-04-28T05:48:49.745620Z","iopub.status.idle":"2023-04-28T05:48:49.759731Z","shell.execute_reply.started":"2023-04-28T05:48:49.745574Z","shell.execute_reply":"2023-04-28T05:48:49.758557Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"encdec=EncoderDecoder(encoder,\n                     decoder,\n                     input_len=2,\n                     target_len=2,\n                     teacher_forcing_prob=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.773273Z","iopub.execute_input":"2023-04-28T05:48:49.774417Z","iopub.status.idle":"2023-04-28T05:48:49.779044Z","shell.execute_reply.started":"2023-04-28T05:48:49.774372Z","shell.execute_reply":"2023-04-28T05:48:49.777819Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"full_seq = (torch.tensor([[-1, -2], [-3, 3], [6, 4], [5, -5]])\n.float()\n.view(1, 4, 2))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.802910Z","iopub.execute_input":"2023-04-28T05:48:49.803988Z","iopub.status.idle":"2023-04-28T05:48:49.808731Z","shell.execute_reply.started":"2023-04-28T05:48:49.803940Z","shell.execute_reply":"2023-04-28T05:48:49.807789Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"encdec.train()\nencdec(full_seq)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.825097Z","iopub.execute_input":"2023-04-28T05:48:49.825826Z","iopub.status.idle":"2023-04-28T05:48:49.839354Z","shell.execute_reply.started":"2023-04-28T05:48:49.825781Z","shell.execute_reply":"2023-04-28T05:48:49.838358Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\n\nforward\nSource Seq : tensor([[[-1., -2.],\n         [-3.,  3.]]])\ntarget Seq : tensor([[[ 6.,  4.],\n         [ 5., -5.]]])\n\n\nBatch_size ->(N) : 1\nInput_length ->(L) : 2 \nEncoder_feature-->(F) : 2\nHidden_seq Of encoder : tensor([[[ 0.0122,  0.3053],\n         [ 0.1228, -0.5392]]], grad_fn=<TransposeBackward1>)\n\n\n***************Decoder Iteration******************\n\n\n\nStore_outputs\nFull Output :  tensor([[[-0.2861,  0.5117]]], grad_fn=<ViewBackward0>)\nIteration 0\nStoring output from 0 : 1\nStored outputs :tensor([[[-0.2861,  0.5117]]], grad_fn=<SliceBackward0>)\n\n\nStore_outputs\nFull Output :  tensor([[[0.0164, 0.4904]]], grad_fn=<ViewBackward0>)\nIteration 1\nStoring output from 1 : 2\nStored outputs :tensor([[[0.0164, 0.4904]]], grad_fn=<SliceBackward0>)\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([[[-0.2861,  0.5117],\n         [ 0.0164,  0.4904]]], grad_fn=<CopySlices>)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Attentions","metadata":{}},{"cell_type":"markdown","source":"## **Values**","metadata":{}},{"cell_type":"markdown","source":"Referring to the encoder’s hidden states (or their affine\ntransformations) as \"values\" (V). The resulting multiplication of a \"value\" by its\ncorresponding attention score is called an alignment vector.the sum of all alignment vectors (that is, the weighted average of the\nhidden states) is called a context vector.","metadata":{}},{"cell_type":"markdown","source":"## **context_vector = $(\\alpha * h_0)+(\\alpha*h_1) = 0.8 * (value)_0 + 0.2 *(value)_1$**","metadata":{}},{"cell_type":"markdown","source":"The encoder’s hidden states are used as both \"keys\" (K)\nand \"values\" (V).will Applying affine transformations to the hidden states,\n**The encoder’s hidden states are called \"keys\" (K), while the\ndecoder’s hidden state is called a \"query\" (Q).**\none for the \"keys,\" another for the \"values,\" so they will actually have different\nvalues.the general idea is that the encoder works like a key-value store, as if it were\nsome sort of database, and then the decoder queries it. The attention mechanism\nlooks the query up in its keys (the matching part) and returns its values\n**The \"query\" (Q) is matched to both \"keys\" (K) to compute the attention scores (s)\nused to compute the context vector, which is simply the weighted average of the\n\"values\" (V).**\n\n## **Context Vector**","metadata":{}},{"cell_type":"code","source":"full_seq = (torch.tensor([[-1, -1], [-1, 1], [1, 1], [1, -1]]).float().view(1, 4, 2))\nsource_seq = full_seq[:, :2]\ntarget_seq = full_seq[:, 2:]","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.844910Z","iopub.execute_input":"2023-04-28T05:48:49.845865Z","iopub.status.idle":"2023-04-28T05:48:49.850885Z","shell.execute_reply.started":"2023-04-28T05:48:49.845798Z","shell.execute_reply":"2023-04-28T05:48:49.849986Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# **values(v)**","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(21)\nencoder=Encoder(n_features=2,hidden_dim=2)\nhidden_seq=encoder(source_seq)\nvalues=hidden_seq #N,L,H\nvalues","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.860206Z","iopub.execute_input":"2023-04-28T05:48:49.860926Z","iopub.status.idle":"2023-04-28T05:48:49.870084Z","shell.execute_reply.started":"2023-04-28T05:48:49.860889Z","shell.execute_reply":"2023-04-28T05:48:49.868962Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.0832, -0.0356],\n         [ 0.3105, -0.5263]]], grad_fn=<TransposeBackward1>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## **keys(k)**","metadata":{}},{"cell_type":"code","source":"keys=hidden_seq #N ,L ,H\nkeys","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.880229Z","iopub.execute_input":"2023-04-28T05:48:49.880589Z","iopub.status.idle":"2023-04-28T05:48:49.887928Z","shell.execute_reply.started":"2023-04-28T05:48:49.880556Z","shell.execute_reply":"2023-04-28T05:48:49.886718Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.0832, -0.0356],\n         [ 0.3105, -0.5263]]], grad_fn=<TransposeBackward1>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Query(Q)**","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(21)\ndecoder=Decoder(n_features=2,hidden_dim=2)\ndecoder.init_hidden(hidden_seq)\ninputs=source_seq[:,-1:]\nout=decoder(inputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.892763Z","iopub.execute_input":"2023-04-28T05:48:49.893192Z","iopub.status.idle":"2023-04-28T05:48:49.902789Z","shell.execute_reply.started":"2023-04-28T05:48:49.893149Z","shell.execute_reply":"2023-04-28T05:48:49.901398Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**The first \"query\" (Q) is the decoder’s hidden state (remember, hidden states are\nalways sequence-first, so we’re permuting it to batch-first)**","metadata":{}},{"cell_type":"code","source":"query=decoder.hidden.permute(1,0,2) # N,1,H\nquery","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.905138Z","iopub.execute_input":"2023-04-28T05:48:49.905590Z","iopub.status.idle":"2023-04-28T05:48:49.915916Z","shell.execute_reply.started":"2023-04-28T05:48:49.905546Z","shell.execute_reply":"2023-04-28T05:48:49.914678Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.3913, -0.6853]]], grad_fn=<PermuteBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"def cal_alphas(ks,q):\n    N,L,H=ks.size()\n    #print(ks.size())\n    alphas=torch.ones(N,1,L).float()*1/L\n    return alphas\nalphas = cal_alphas(keys,query)\nalphas","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.918038Z","iopub.execute_input":"2023-04-28T05:48:49.918754Z","iopub.status.idle":"2023-04-28T05:48:49.927463Z","shell.execute_reply.started":"2023-04-28T05:48:49.918706Z","shell.execute_reply":"2023-04-28T05:48:49.926158Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"tensor([[[0.5000, 0.5000]]])"},"metadata":{}}]},{"cell_type":"code","source":"# N, 1, L x N, L, H -> 1, L x L, H -> 1, H\ncontext_vector = torch.bmm(alphas, values)\ncontext_vector","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.929478Z","iopub.execute_input":"2023-04-28T05:48:49.930115Z","iopub.status.idle":"2023-04-28T05:48:49.937508Z","shell.execute_reply.started":"2023-04-28T05:48:49.930081Z","shell.execute_reply":"2023-04-28T05:48:49.936531Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.1968, -0.2809]]], grad_fn=<BmmBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"**context vector is ready to  concatenate it to the \"query\" (the\ndecoder’s hidden state) and use it as the input for the linear layer that actually\ngenerates the predicted**","metadata":{}},{"cell_type":"code","source":"concatenated = torch.cat([context_vector, query], axis=-1)\nconcatenated","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.939566Z","iopub.execute_input":"2023-04-28T05:48:49.940333Z","iopub.status.idle":"2023-04-28T05:48:49.954060Z","shell.execute_reply.started":"2023-04-28T05:48:49.940275Z","shell.execute_reply":"2023-04-28T05:48:49.952426Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.1968, -0.2809,  0.3913, -0.6853]]], grad_fn=<CatBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"**scoring method will use the transformed \"keys\" and \"queries\" to compute\nthe attention scores,**","metadata":{}},{"cell_type":"markdown","source":"                          \n\n**keys(k) : ---->Encoder-->Affine Transforms-->Scoring**          \n\n**Queries(Q) : ----->Decoder--->Affine Transforms--->scroing**        \n\n**Values(v) :---->Encoder---->Alingnment Vector**","metadata":{}},{"cell_type":"markdown","source":"## Scoring Method\n\n**A \"key\" (K) is a hidden state from the encoder. A \"query\" (Q) is a hidden state from\nthe decoder. Both of them are vectors with the same number of dimensions If two vectors are\npointing in the same direction, their cosine similarity is a perfect one.**","metadata":{}},{"cell_type":"markdown","source":"$\\cos\\theta \\left\\lVert Q \\right\\rVert \\left\\lVert K \\right\\rVert = Q .K $","metadata":{}},{"cell_type":"code","source":"product=torch.bmm(query,keys.permute(0,2,1))\nproduct","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.956037Z","iopub.execute_input":"2023-04-28T05:48:49.956808Z","iopub.status.idle":"2023-04-28T05:48:49.965975Z","shell.execute_reply.started":"2023-04-28T05:48:49.956772Z","shell.execute_reply":"2023-04-28T05:48:49.965034Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"tensor([[[0.0569, 0.4821]]], grad_fn=<BmmBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Attention Scores**","metadata":{}},{"cell_type":"code","source":"alphas =F.softmax(product,dim=-1)\nalphas","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.967243Z","iopub.execute_input":"2023-04-28T05:48:49.967979Z","iopub.status.idle":"2023-04-28T05:48:49.975240Z","shell.execute_reply.started":"2023-04-28T05:48:49.967932Z","shell.execute_reply":"2023-04-28T05:48:49.974357Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"tensor([[[0.3953, 0.6047]]], grad_fn=<SoftmaxBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"The attention scores above mean that\nthe first hidden state(0.3953) contributes to roughly 40% of the context vector while the second hidden state(0.60) contributes to the remaining 60% of the context vector.","metadata":{}},{"cell_type":"markdown","source":"## **Updating $\\alpha$**","metadata":{}},{"cell_type":"code","source":"def calc_alphas(ks,q):\n    \n    product=torch.bmm(q,ks.permute(0,2,1))\n    alphas=F.softmax(produt,dim=-1)\n    return alphas","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.977400Z","iopub.execute_input":"2023-04-28T05:48:49.978349Z","iopub.status.idle":"2023-04-28T05:48:49.983361Z","shell.execute_reply.started":"2023-04-28T05:48:49.978298Z","shell.execute_reply":"2023-04-28T05:48:49.982255Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## **Scaled Dot Product**\nSo far, we’ve used simple dot products between a \"query\" and each of the \"keys.\"\nBut, given that the dot product between two vectors is the sum of the elements\nafter an element-wise multiplication of both vectors, guess what happens as the\nvectors grow to a larger number of dimensions? The variance gets larger as well.\nSo, we need to (somewhat) standardize it by scaling the dot product by the inverse\nof its standard deviation:\n\n**Scaled dot Product** = $Q.K / \\sqrt(d_k)$","metadata":{}},{"cell_type":"code","source":"dims = query.size(-1)\nscaled_products = products / np.sqrt(dims)\nscaled_products","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:49.985254Z","iopub.execute_input":"2023-04-28T05:48:49.986096Z","iopub.status.idle":"2023-04-28T05:48:50.008117Z","shell.execute_reply.started":"2023-04-28T05:48:49.985981Z","shell.execute_reply":"2023-04-28T05:48:50.006519Z"},"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/493885050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscaled_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproducts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscaled_products\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'products' is not defined"],"ename":"NameError","evalue":"name 'products' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def calc_alphas(ks, q):\n    dims = q.size(-1)\n    # N, 1, H x N, H, L -> N, 1, L\n    products = torch.bmm(q, ks.permute(0, 2, 1))\n    scaled_products = products / np.sqrt(dims)\n    alphas = F.softmax(scaled_products, dim=-1)\n    return alphas\nalphas = calc_alphas(keys, query)\n# N, 1, L x N, L, H -> 1, L x L, H -> 1, H\ncontext_vector = torch.bmm(alphas, values)\ncontext_vector","metadata":{"execution":{"iopub.status.busy":"2023-04-28T05:48:50.009216Z","iopub.status.idle":"2023-04-28T05:48:50.009965Z","shell.execute_reply.started":"2023-04-28T05:48:50.009739Z","shell.execute_reply":"2023-04-28T05:48:50.009765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attention**\n              +-------------+\n              |             |\n              |   Query q   |\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              | Key vectors |\n              |     ks      |\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              |    Linear   |\n              | Transformation|\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              |  Key matrix |\n              |     K       |\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              |  Value matrix|\n              |     V       |\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              | Product and |\n              |   Scaling   |\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              |   Softmax   |\n              |             |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              |  Attention  |\n              |   weights   |\n              |    alphas   |\n              +------+------+\n                     |\n                     v\n              +-------------+\n              |             |\n              |  Weighted   |\n              |  sum of the |\n              |   values    |\n              +------+------+\n                     |\n                     v\n                +-------+\n                |       |\n                |  alphas x  |\n                |   values|\n                |       |\n                +-------+\n                     |\n                     v\n              +-------------+\n              |             |\n              | Context vec.|\n              |             |\n              +------+------+\n                     |\n                     v\n                Output\n","metadata":{}},{"cell_type":"markdown","source":"## **Mask**","metadata":{}},{"cell_type":"markdown","source":"mask some of the \"values\" to force the attention\nmechanism to ignore them.\n**The mask will make the attention score equal to zero for the padded data points.**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    \n    def __init__(self,hidden_dim,input_dim=None,proj_values=False):\n        \n        super().__init__()\n        self.d_k=hidden_dim\n        self.input_dim=hidden_dim if input_dim is None else input_dim\n        \n        self.proj_values=proj_values\n        \n        self.linear_query=nn.Linear(self.input_dim,hidden_dim)\n        self.linear_key=nn.Linear(self.input_dim,hidden_dim)\n        self.linear_value=nn.Linear(self.input_dim,hidden_dim)\n        \n        self.alphas=None\n        \n    def init_keys(self, keys):\n        print('\\n\\nkey Intialisation')\n        self.keys = keys\n        print(f'\\nkeys : {self.keys}')\n        self.proj_keys = self.linear_key(self.keys)\n        \n        self.values = self.linear_value(self.keys) if self.proj_values else self.keys\n        print(f'\\nValues : {self.values}')\n    def scores_function(self,query):\n        \n        proj_query=self.linear_query(query)\n        \n        dot_products=torch.bmm(proj_query,self.proj_keys.permute(0,2,1))\n        print(f'\\nQ . K : {dot_products}')\n        scores=dot_products/np.sqrt(self.d_k)\n        print(f'\\nScores :{scores}')\n        return scores\n    \n    def forward(self,query,mask=None):\n        scores=self.scores_function(query)\n        if mask is not None:\n            scores=scores.masked_fill(mask==0,-1e9)\n        print(f'\\nMask : {mask}')\n        alphas=F.softmax(scores,dim=-1)\n        self.alphas=alphas.detach()\n        print(f'\\nAlphas : {self.alphas}')\n        context=torch.bmm(alphas,self.values)\n        return context\n    \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:18:31.955585Z","iopub.execute_input":"2023-04-28T06:18:31.956532Z","iopub.status.idle":"2023-04-28T06:18:31.971959Z","shell.execute_reply.started":"2023-04-28T06:18:31.956475Z","shell.execute_reply":"2023-04-28T06:18:31.970430Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"source_seq = torch.tensor([[[-1., 1.], [0., 0.]]])\n# pretend there's an encoder here...\nkeys = torch.tensor([[[-.38, .44], [.85, -.05]]])\nquery = torch.tensor([[[-1., 1.]]])","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:18:32.449514Z","iopub.execute_input":"2023-04-28T06:18:32.449927Z","iopub.status.idle":"2023-04-28T06:18:32.456257Z","shell.execute_reply.started":"2023-04-28T06:18:32.449891Z","shell.execute_reply":"2023-04-28T06:18:32.454883Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"source_mask = (source_seq != 0).all(axis=2).unsqueeze(1)\nsource_mask","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:18:33.056146Z","iopub.execute_input":"2023-04-28T06:18:33.057536Z","iopub.status.idle":"2023-04-28T06:18:33.066210Z","shell.execute_reply.started":"2023-04-28T06:18:33.057474Z","shell.execute_reply":"2023-04-28T06:18:33.065093Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"tensor([[[ True, False]]])"},"metadata":{}}]},{"cell_type":"code","source":"torch.manual_seed(11)\nattnh = Attention(2)\nattnh.init_keys(keys)\ncontext = attnh(query, mask=source_mask)\nprint(f'\\nContext Vector :',attnh.alphas,context)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:19:08.678734Z","iopub.execute_input":"2023-04-28T06:19:08.679663Z","iopub.status.idle":"2023-04-28T06:19:08.690349Z","shell.execute_reply.started":"2023-04-28T06:19:08.679619Z","shell.execute_reply":"2023-04-28T06:19:08.689351Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"\n\nkey Intialisation\n\nkeys : tensor([[[-0.3800,  0.4400],\n         [ 0.8500, -0.0500]]])\n\nValues : tensor([[[-0.3800,  0.4400],\n         [ 0.8500, -0.0500]]])\n\nQ . K : tensor([[[-0.9101, -0.5215]]], grad_fn=<BmmBackward0>)\n\nScores :tensor([[[-0.6436, -0.3688]]], grad_fn=<DivBackward0>)\n\nMask : tensor([[[ True, False]]])\n\nAlphas : tensor([[[1., 0.]]])\n\nContext Vector : tensor([[[1., 0.]]]) tensor([[[-0.3800,  0.4400]]], grad_fn=<BmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Decoder Attention","metadata":{}},{"cell_type":"code","source":"class DecoderAttn(nn.Module):\n    \n    def __init__(self,n_features,hidden_dim):\n        \n        super().__init__()\n        self.hidden_dim=hidden_dim\n        self.n_features=n_features\n        self.hidden=None\n        self.basic_rnn=nn.GRU(self.n_features,\n                             self.hidden_dim,\n                             batch_first=True)\n        print('\\nEncoder Attention :-')\n        self.attn=Attention(self.hidden_dim)\n        self.regression=nn.Linear(2*self.hidden_dim,self.n_features)\n        \n    def init_hidden(self,hidden_seq):\n        \n        self.attn.init_keys(hidden_seq)\n        \n        hidden_final=hidden_seq[:,-1:]\n        self.hidden=hidden_final.permute(1,0,2)\n    def forward(self,x,mask=None):\n        \n        batch_first_output,self.hidden=self.basic_rnn(x,self.hidden)\n        query=batch_first_output[:,-1:]\n        print(f'\\nQuery : {query}')\n        #Attention\n        print('\\nAttention Outputs :')\n        context=self.attn(query,mask=mask)\n        print(f'\\nContext Vector: ',context)\n        concatenated=torch.cat([context,\n                               query],axis=-1)\n        print('\\nLinear Layer Output :')\n        out=self.regression(concatenated)\n        return out.view(-1,1,self.n_features)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:23:00.812812Z","iopub.execute_input":"2023-04-28T06:23:00.813904Z","iopub.status.idle":"2023-04-28T06:23:00.824502Z","shell.execute_reply.started":"2023-04-28T06:23:00.813853Z","shell.execute_reply":"2023-04-28T06:23:00.823213Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"full_seq = (torch.tensor([[-1, -1], [-1, 1], [1, 1], [1, -1]])\n.float()\n.view(1, 4, 2))\nsource_seq = full_seq[:, :2]\ntarget_seq = full_seq[:, 2:]","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:23:01.503823Z","iopub.execute_input":"2023-04-28T06:23:01.504251Z","iopub.status.idle":"2023-04-28T06:23:01.511385Z","shell.execute_reply.started":"2023-04-28T06:23:01.504213Z","shell.execute_reply":"2023-04-28T06:23:01.510235Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(21)\nencoder = Encoder(n_features=2, hidden_dim=2)\ndecoder_attn = DecoderAttn(n_features=2, hidden_dim=2)\n# Generates hidden states (keys and values)\nhidden_seq = encoder(source_seq)\ndecoder_attn.init_hidden(hidden_seq)\n# Target sequence generation\ninputs = source_seq[:, -1:]\ntarget_len = 2\nfor i in range(target_len):\n    out = decoder_attn(inputs)\n    print(f'\\n\\033[1m\\033[36m\\033[4m Final Output: {out} \\033[0m')\n    inputs = out","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:25:48.490297Z","iopub.execute_input":"2023-04-28T06:25:48.491572Z","iopub.status.idle":"2023-04-28T06:25:48.507547Z","shell.execute_reply.started":"2023-04-28T06:25:48.491521Z","shell.execute_reply":"2023-04-28T06:25:48.506353Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"\nEncoder Attention :-\n\n\nkey Intialisation\n\nkeys : tensor([[[ 0.0832, -0.0356],\n         [ 0.3105, -0.5263]]], grad_fn=<TransposeBackward1>)\n\nValues : tensor([[[ 0.0832, -0.0356],\n         [ 0.3105, -0.5263]]], grad_fn=<TransposeBackward1>)\n\nQuery : tensor([[[0.0930, 0.2894]]], grad_fn=<SliceBackward0>)\n\nAttention Outputs :\n\nQ . K : tensor([[[-0.3376, -0.3259]]], grad_fn=<BmmBackward0>)\n\nScores :tensor([[[-0.2387, -0.2304]]], grad_fn=<DivBackward0>)\n\nMask : None\n\nAlphas : tensor([[[0.4979, 0.5021]]])\n\nContext Vector:  tensor([[[ 0.1973, -0.2820]]], grad_fn=<BmmBackward0>)\n\nLinear Layer Output :\n\n\u001b[1m\u001b[36m\u001b[4m Final Output: tensor([[[-0.3555, -0.1220]]], grad_fn=<ViewBackward0>) \u001b[0m\n\nQuery : tensor([[[-0.1302,  0.4045]]], grad_fn=<SliceBackward0>)\n\nAttention Outputs :\n\nQ . K : tensor([[[-0.4355, -0.4056]]], grad_fn=<BmmBackward0>)\n\nScores :tensor([[[-0.3079, -0.2868]]], grad_fn=<DivBackward0>)\n\nMask : None\n\nAlphas : tensor([[[0.4947, 0.5053]]])\n\nContext Vector:  tensor([[[ 0.1980, -0.2835]]], grad_fn=<BmmBackward0>)\n\nLinear Layer Output :\n\n\u001b[1m\u001b[36m\u001b[4m Final Output: tensor([[[-0.2641, -0.2521]]], grad_fn=<ViewBackward0>) \u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}